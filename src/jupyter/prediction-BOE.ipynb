{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict the demographics of Twitter users based on the emojis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import confusion_matrix, log_loss, f1_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_df_path = 'usage_per_user/'\n",
    "\n",
    "with open(parent_df_path+'joh_df.pkl', 'rb') as f:\n",
    "    joh_per_usage = pickle.load(f)\n",
    "    \n",
    "with open(parent_df_path+'lon_df.pkl', 'rb') as f:\n",
    "    lon_per_usage = pickle.load(f)\n",
    "    \n",
    "with open(parent_df_path+'nyc_df.pkl', 'rb') as f:\n",
    "    nyc_per_usage = pickle.load(f)\n",
    "    \n",
    "with open(parent_df_path+'ran_df.pkl', 'rb') as f:\n",
    "    ran_per_usage = pickle.load(f)\n",
    "\n",
    "\n",
    "all_usage = pd.concat([joh_per_usage,lon_per_usage,nyc_per_usage,ran_per_usage],ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>#Ô∏è‚É£</th>\n",
       "      <th>*Ô∏è‚É£</th>\n",
       "      <th>0Ô∏è‚É£</th>\n",
       "      <th>1Ô∏è‚É£</th>\n",
       "      <th>2Ô∏è‚É£</th>\n",
       "      <th>3Ô∏è‚É£</th>\n",
       "      <th>4Ô∏è‚É£</th>\n",
       "      <th>5Ô∏è‚É£</th>\n",
       "      <th>6Ô∏è‚É£</th>\n",
       "      <th>7Ô∏è‚É£</th>\n",
       "      <th>...</th>\n",
       "      <th>üß∫</th>\n",
       "      <th>üßª</th>\n",
       "      <th>üßº</th>\n",
       "      <th>üßΩ</th>\n",
       "      <th>üßæ</th>\n",
       "      <th>üßø</th>\n",
       "      <th>tweets_contain_emoji</th>\n",
       "      <th>total_tweets</th>\n",
       "      <th>gender</th>\n",
       "      <th>ethnicity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>62.0</td>\n",
       "      <td>111.0</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>566.0</td>\n",
       "      <td>945.0</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>251.0</td>\n",
       "      <td>444.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>410.0</td>\n",
       "      <td>887.0</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18684</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>117.0</td>\n",
       "      <td>3121.0</td>\n",
       "      <td>female</td>\n",
       "      <td>asian/hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18685</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>551.0</td>\n",
       "      <td>3072.0</td>\n",
       "      <td>female</td>\n",
       "      <td>black</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18686</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1055.0</td>\n",
       "      <td>2482.0</td>\n",
       "      <td>male</td>\n",
       "      <td>asian/hispanic</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18687</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>905.0</td>\n",
       "      <td>2752.0</td>\n",
       "      <td>female</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18688</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>15.0</td>\n",
       "      <td>3206.0</td>\n",
       "      <td>male</td>\n",
       "      <td>white</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>18689 rows √ó 2788 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       #Ô∏è‚É£  *Ô∏è‚É£  0Ô∏è‚É£  1Ô∏è‚É£  2Ô∏è‚É£  3Ô∏è‚É£  4Ô∏è‚É£  5Ô∏è‚É£  6Ô∏è‚É£  7Ô∏è‚É£  ...    üß∫    üßª    üßº  \\\n",
       "0      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "1      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "2      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "3      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "4      0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "...    ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...  ...   \n",
       "18684  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "18685  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "18686  0.0  0.0  0.0  1.0  0.0  1.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "18687  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "18688  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  0.0  ...  0.0  0.0  0.0   \n",
       "\n",
       "         üßΩ    üßæ    üßø  tweets_contain_emoji  total_tweets  gender  \\\n",
       "0      0.0  0.0  0.0                  62.0         111.0  female   \n",
       "1      0.0  0.0  0.0                   0.0           2.0  female   \n",
       "2      0.0  0.0  0.0                 566.0         945.0  female   \n",
       "3      0.0  0.0  0.0                 251.0         444.0    male   \n",
       "4      0.0  0.0  0.0                 410.0         887.0  female   \n",
       "...    ...  ...  ...                   ...           ...     ...   \n",
       "18684  0.0  0.0  0.0                 117.0        3121.0  female   \n",
       "18685  0.0  0.0  0.0                 551.0        3072.0  female   \n",
       "18686  0.0  0.0  0.0                1055.0        2482.0    male   \n",
       "18687  0.0  0.0  0.0                 905.0        2752.0  female   \n",
       "18688  0.0  0.0  0.0                  15.0        3206.0    male   \n",
       "\n",
       "            ethnicity  \n",
       "0               black  \n",
       "1               black  \n",
       "2               black  \n",
       "3               white  \n",
       "4               black  \n",
       "...               ...  \n",
       "18684  asian/hispanic  \n",
       "18685           black  \n",
       "18686  asian/hispanic  \n",
       "18687           white  \n",
       "18688           white  \n",
       "\n",
       "[18689 rows x 2788 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_usage.drop(labels = all_usage[all_usage.ethnicity.eq('other')].index,inplace =True)\n",
    "all_usage.reset_index(inplace = True,drop = True)\n",
    "\n",
    "all_usage[\"ethnicity\"].replace({\"asian\":\"asian/hispanic\",\"hispanic\":\"asian/hispanic\"}, inplace=True)\n",
    "all_usage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train/test set split: 80%/20% "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "emoji_usage = all_usage.drop(columns = ['tweets_contain_emoji','total_tweets','gender','ethnicity'])\n",
    "\n",
    "X_train_gender, X_test_gender, y_train_gender, y_test_gender = train_test_split(emoji_usage, all_usage['gender'], test_size=0.2, random_state=43)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gender predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.692     0.823     0.752      1586\n",
      "        male      0.849     0.730     0.785      2152\n",
      "\n",
      "    accuracy                          0.770      3738\n",
      "   macro avg      0.770     0.777     0.768      3738\n",
      "weighted avg      0.782     0.770     0.771      3738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_linear = LinearSVC(dual = False,tol=1e-2,max_iter = 10000,penalty = 'l2').fit(X_train_gender, y_train_gender) #l2\n",
    "report_SVMlinear = classification_report(svm_linear.predict(X_test_gender),y_test_gender,digits = 3)\n",
    "print(report_SVMlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.793     0.791     0.792      1892\n",
      "        male      0.786     0.788     0.787      1846\n",
      "\n",
      "    accuracy                          0.789      3738\n",
      "   macro avg      0.789     0.789     0.789      3738\n",
      "weighted avg      0.789     0.789     0.789      3738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(criterion='entropy', max_depth=100, n_estimators=500).fit(X_train_gender, y_train_gender)\n",
    "report_rf = classification_report(rf.predict(X_test_gender),y_test_gender,digits = 3)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      female      0.787     0.819     0.802      1814\n",
      "        male      0.822     0.791     0.806      1924\n",
      "\n",
      "    accuracy                          0.804      3738\n",
      "   macro avg      0.805     0.805     0.804      3738\n",
      "weighted avg      0.805     0.804     0.804      3738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GBC = GradientBoostingClassifier(n_estimators=300,max_depth = 7).fit(X_train_gender, y_train_gender)\n",
    "report_GBC = classification_report(GBC.predict(X_test_gender),y_test_gender,digits = 3)\n",
    "print(report_GBC)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ethnicity predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_eth, X_test_eth, y_train_eth, y_test_eth = train_test_split(all_usage.drop(columns = ['tweets_contain_emoji','total_tweets','gender','ethnicity']), all_usage['ethnicity'], test_size=0.2, random_state=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "asian/hispanic      0.411     0.524     0.461       248\n",
      "         black      0.773     0.914     0.838      1281\n",
      "         white      0.906     0.783     0.840      2209\n",
      "\n",
      "      accuracy                          0.811      3738\n",
      "     macro avg      0.697     0.740     0.713      3738\n",
      "  weighted avg      0.828     0.811     0.814      3738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "svm_linear = LinearSVC(class_weight = 'balanced',tol=1e-2,penalty = 'l2',dual = False,max_iter=20000).fit(X_train_eth, y_train_eth)\n",
    "report_SVMlinear = classification_report(svm_linear.predict(X_test_eth),y_test_eth,digits = 3,zero_division = 0)\n",
    "print(report_SVMlinear)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "asian/hispanic      0.304     0.828     0.444       116\n",
      "         black      0.821     0.897     0.858      1385\n",
      "         white      0.928     0.792     0.855      2237\n",
      "\n",
      "      accuracy                          0.832      3738\n",
      "     macro avg      0.684     0.839     0.719      3738\n",
      "  weighted avg      0.869     0.832     0.843      3738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "rf = RandomForestClassifier(n_estimators=500, criterion='entropy',max_depth=70,class_weight = 'balanced').fit(X_train_eth, y_train_eth)\n",
    "report_rf = classification_report(rf.predict(X_test_eth),y_test_eth,digits = 3)\n",
    "print(report_rf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                precision    recall  f1-score   support\n",
      "\n",
      "asian/hispanic      0.383     0.766     0.511       158\n",
      "         black      0.844     0.900     0.871      1420\n",
      "         white      0.921     0.814     0.864      2160\n",
      "\n",
      "      accuracy                          0.845      3738\n",
      "     macro avg      0.716     0.827     0.749      3738\n",
      "  weighted avg      0.869     0.845     0.852      3738\n",
      "\n"
     ]
    }
   ],
   "source": [
    "GBC = GradientBoostingClassifier(n_estimators=300,max_depth = 7).fit(X_train_eth, y_train_eth)\n",
    "report_GBC = classification_report(GBC.predict(X_test_eth),y_test_eth,digits = 3)\n",
    "print(report_GBC)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
